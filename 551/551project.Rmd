---
title: "551 Project"
author: "Chen Xie, Xinye Jiang, Xun Wang"
date: "2019/4/13"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,message=FALSE,warning=FALSE)
```

## 1 Introduction

Logistic regression is a very famous and widely applied technique in supervised learning. It is usually used to perform predictive analysis when the response variable is binary. 

Logistic regression can also be approached by Bayesian modeling. In general, Bayesian analysis is more flexible, and it is proved to be superior for small samples. For Bayesian modeling, it can incorporate prior information. For example, if we want to know exactly which factors are most effective on the response, we can use shrinkage prior to implement variable selection. 

In practice, predicting the exit status (binary: 0 or 1) of customers of a bank can be an application of the standard logistic regression as well as Bayesian approach. In this report, we delve into a data set about account status of a bank in three European countries. The objective is to predict if the clients will leave the bank based on part of their information, such as geography, gender and account balances, etc. In this process, we will also explore the different effects of specific predictors to response variable in different models. 

## 2 Data Exploration

### 2.1 Data Set

The data set we use for this report is from Kaggle website. It has 10000 observations and 11 variables. The Exit Status (Exited) is our response variable. The Exit States is 1 if the customer closed account with bank and 0 if the customer is retained. The other variables can be treated as predictors, and they are Credit Score, Geography, Gender, Age, Tenure, Balance, Number Of Products (NumOfProducts), Has Credit Card (HasCrCard), Is Active Member, Estimated Salary. This data set has mixture of types of independent variables, where 6 of them are numeric and 4 are categorical. And the Table 1 below shows more details of all the variables in this data, including names, types and brief descriptions.


```{r}
library(MCMCpack); library(coda); library(EBglmnet)
library(ggplot2);library(gbm);library(GGally)
library(dplyr);library(tidyr)
library(knitr);library(kableExtra)
c = read.csv("Churn_Modelling.csv")[,-c(1:3)]
c$Geography = as.factor(c$Geography)
c$Gender = as.factor(c$Gender)
c$HasCrCard = as.factor(c$HasCrCard)
c$IsActiveMember = as.factor(c$IsActiveMember)
c$Exited = as.factor(c$Exited)
```
 
 
```{r,out.height='80%',out.width='80%'}
library(tidyr)
cap='Brief Descriptions of Predictors'
data.frame(v1=c('Exited Status','Credit Score','Geography','Gender',
                'Age','Tenure','Balance','Number of Products',
                'Has Credit Card','Is Active Member','Estimated Salary'),
       v2=c('Binary','Continuous','Categorical','Binary',
            'Continuous','Continuous','Continuous',
            'Discrete','Binary','Binary','Continuous'),
       v3=c('1 if the customer closed account and 0 if the customer is retained',
            'Credit Score of the customer',
            'The country from which the customer belongs',
            'Male or Female','Age of the customer',
            'Number of years for which the customer has been with the bank',
            'Bank balance of the customer',
            'Number of bank products the customer is utilizing',
            'Whether the customer holds a credit card with the bank or not',
            'Whether the customer is an active member with the bank or not',
            'Estimated salary of the customer in Dollars'
            ))%>%
  knitr::kable(col.names = c('Variables','Type','Description'),
               caption = cap,align = 'l',format="latex",longtable=TRUE,booktabs = T) %>%
  kableExtra::kable_styling(latex_options = "striped") 
```

### 2.2 Data Exploration

The Table 2 and Table 3 are summary statistics of all the variables. To be more precise, Figure 1 shows barcharts of categorical variables. In the Figure 1, the left figure shows the overall distribution of Exited statu, where we could find that a large proportion of clients keep their bank accounts. The number of Exited Status=0 is almost four times of number of clients who left the bank. The data is very imbalanced, which may be problems for further inference or prediction. The right part in Figure 1 shows how categorical predictors are distributed by Exited Status. It implies that whether the customer has a credit card extremely affects the Exited Status.

```{r}
do.call(cbind, lapply(c[,c(1,4,5,6,7,10)],summary)) %>%
  kable("latex", booktabs = T,
        caption='Summary of Numeric Variables') %>%
  kable_styling(latex_options = "striped")

tbl1<-tibble('Geography'=c('France: 5014','Germany: 2509', 'Spain: 2477'), 
             'Gender '=c('Female: 4543', 'Male: 5457',''), 
             'HasCrCard'=c('0: 2945', '1: 7055 ',''),
             'IsActiveMember'=c('0: 4849', '1: 5151',''),
             'Exited'=c('0: 7963', '1: 2037',''))
tbl1 %>%
  kable("latex", booktabs = T,
        caption = 'Summary of Categorical Variables') %>%
  kable_styling()
```


```{r, echo=FALSE,out.width="49%", out.height="30%",fig.cap="Barcharts",fig.show='hold',fig.align='center'}
knitr::include_graphics(c("response1.png","response2.png"))

```


Next, we also want to explore the relationship between predictors and the response variable. The left part in Figure 2 shows the boxplots of continuous factors by Exited Status. It provides some evidence that some predictors have strong impact on Exited Status. For instance, Age and Balance are inluential factors for Exited Status. The right part in Figure 2  is the pairwise plot of numeric predictors, including scatterplots, correlations bewteen each two of them, as well as histograms. We could gain a general knowledge of distributions for every numeric variable. According to the right plot in Figure 2, the numeric predicors are not highly correlated with each other. 

However, although we can observe some information of the relationship between predictors and the response variable, we need more credible proofs.

```{r, echo=FALSE,out.width="49%", out.height="30%",fig.cap="Boxplots and Scatter Plots",fig.show='hold',fig.align='center'}
knitr::include_graphics(c("boxplot.png","pairs.png"))

```


### 2.3 Problems of the Data

Imbalanced data unclear source biased data collection It may need more ... we can still do some learning from this data.

## 3 Regression Models

In this part, we will briefly introduce three logistic regression models, including the standard one and two Bayesian models. 

### 3.1 Logistic Regression

In addition to the most significant reason that our response variable is binary, there are several advantages of logistic regression against other methods. First, it can be interpreted more easily comapred with some complex models, while holding the same level of prediction precision. It helps to explain the relationship between the predictors and response variable. Next, the logistic regression can also handle mixed types of exploratory variables. Finally, logistic regression is an informative method that it provides both the size and the direction of the effects of its predictors.

The basic assumption of standard logistic regression model is that the observations $y_1, ..., y_n$ are independent and following binomial distribution (1, $p_i$), where $p_i$ is the probability of response $y_i=1$. We could estimate the parameters $\beta$ by maximum likelihood method, that is, maximizing $P(y_1,...,y_n;\beta)$. The estimate is also called ML estimate. The statistical description is as below:

\[y_1,...,y_n, \textup{ are independent}\]

\[y_i\sim \textup{ Binomial }(1, p_i)\]

\[\log(\frac{p_i}{1-p_i})=x_i^T\beta\]

\[p(y_i;\beta)=(p_i)^{y_i}(1-p_i)^{1-y_i}\]

\[p(y_1,...,y_n;\beta)=\prod_{i=1}^n(p_i)^{y_i}(1-p_i)^{1-y_i}\]

Among them, $x_i$ is the i-th row of observation and $\beta$ is the  parameters.

### 3.2 Bayesian Logistic Regression with Normal prior

In the standard logistic regression above, we treat $\beta$ as a column of unknown but fixed parameters. Actually, $\beta$ could be seen as a vector of random variables from the prospective of Bayesian analysis. That is the preliminary thought of Bayesian analysis. In this case, we can also get a point estimate for the parameters by maximizing the posterior distribution. The maximum of posterior distribution, that is, the mode of posterior, is called MAP estimation. But sometimes, it is hard to get analytic form of mode of the posterior distribution. Instead, we can estimate the parameters by mean or median of posterior samples, which is ususally generated by Markov chain Monte Carlo (MCMC) sampling methods. For this paper, we also use this idea for inference and prediction. 

In the Bayesian logistic regression, basiclly it assumes that $\beta$ follows a multivariate normal prior distribution. For simplicity, we also assume $\beta_i$ are indenpendent with each other. and $\mu$, $\sigma$. It has some shrinkage effects on $\beta$, but not strong. 

The assumptions are:

\[y_1,...,y_n, \textup{ are independent}\]

\[\textup{Likelihood: } y_i\sim \textup{ Binomial }(1, p_i)\]

\[\textup{Parameters: }\log(\frac{p_i}{1-p_i})=x_i^T\beta\]

\[\textup{Prior: } \beta\sim N(\mathbf{\mu,\Sigma} )\]

\[\textup{Also Assume: } \beta_1,...,\beta_p \textup{ are independent}\]

\[\beta_i\sim N(0,0.01^{-1} )\]

where $\mu$ is the mean vector of $\beta$ and $\Sigma$ is a covariance matrix.



### 3.3 Bayesian Logistic Regression with NE prior

To figure out the most important predictors to the exited status response and to improve the prediction precision, we could use the LASSO Interpretation, cross validation methods,...

\[y_1,...,y_n, \textup{ are independent}\]

\[\textup{Likelihood: } y_i\sim \textup{ Binomial }(1, p_i)\]

\[\textup{Parameters: }\log(\frac{p_i}{1-p_i})=x_i^T\beta\]

\[\textup{prior: } \beta_i\sim N(0,\sigma_i^2)\]

\[\textup{Assume: } \beta_1,...,\beta_p \textup{ are independent}\]

\[\textup{Hyper Prior: } P(\sigma_i^2) \sim \textup{Exponential }(\lambda)\]

```{r}
library(tidyr); library(MCMCpack); library(coda); library(EBglmnet); library(GGally); library(ggplot2)
c = read.csv("Churn_Modelling.csv")[,-c(1:3)]
c$Geography = as.factor(c$Geography)
c$Gender = as.factor(c$Gender)
c$HasCrCard = as.factor(c$HasCrCard)
c$IsActiveMember = as.factor(c$IsActiveMember)
c$Exited = as.factor(c$Exited)
```

```{r,eval=FALSE}
c_pairs=ggpairs(c[c(1,4,5,6,7,10)],axisLabels = "none",
        upper = list(continuous = "points", combo = "dot"),
        lower = list(continuous = "cor", combo = "dot"),
        diag = list(continuous = "barDiag"),
        title='Figure 1. Pairwise scatterplots and histogram of numeric variables.') + 
  theme_bw()
c_pairs
```

```{r,eval=FALSE}
library(gbm)
p1=ggplot(c, aes(x=Geography,y=Exited,color=Geography)) +
  geom_boxplot(outlier.colour="black", outlier.shape=16,outlier.size=2, notch=FALSE)+
  ggtitle("Boxplot of Geography")+
  scale_color_manual(values=c("#999999", "#E69F00", "#56B4E9"))
p2=ggplot(c, aes(x=Gender,y=Exited,color=Gender)) +
  geom_boxplot(outlier.colour="black", outlier.shape=16,outlier.size=2, notch=FALSE)+
  ggtitle("Boxplot of Gender")+
  scale_color_manual(values=c("#999999", "#E69F00", "#56B4E9"))
p3=ggplot(c, aes(x=HasCrCard,y=Exited,color=HasCrCard)) +
  geom_boxplot(outlier.colour="black", outlier.shape=16,outlier.size=2, notch=FALSE)+
  ggtitle("Boxplot of HasCrCard")+
  scale_color_manual(values=c("#999999", "#E69F00", "#56B4E9"))
p4=ggplot(c, aes(x=IsActiveMember,y=Exited,color=IsActiveMember)) +
  geom_boxplot(outlier.colour="black", outlier.shape=16,outlier.size=2, notch=FALSE)+
  ggtitle("Boxplot of IsActiveMember")+
  scale_color_manual(values=c("#999999", "#E69F00", "#56B4E9"))
grid.arrange(p1, p2, p3,p4,nrow = 2)
```

```{r}
# Split into training & test sets
C = model.matrix(Exited~., data=c)
set.seed(3)
ind = sample(1:10000, 7500, replace=F)
Xtrain = C[ind,]
Xtest = C[-ind,]
ytrain = c[ind,11]
ytest = c[-ind,11]
```


Standard logistic regression could be easily fitted by using a very common function `glm` in `stats` package.
Printing out the summary table, we get a model like:

```{r}
### Logistic Regression
res_original = glm(ytrain~., data=as.data.frame(Xtrain[,-1]), family=binomial(link="logit"))
summary(res_original)
### prediction of Standard logistic regression
p1 = summary(res_original)$coefficients[,4]
beta1 = res_original$coefficients
name = names(beta1)
predtest1 = 1*(predict(res_original, newdata=as.data.frame(Xtest[,-1]), type="response")>=0.5)
table(ytest, predtest1)
### right rate
pred1 = mean(predtest1==ytest)
```

Intepretation of parameters will be showed in next part together with other two models.

Monte Carlo Markov Chain is an extremely effective way to sample from posterior distribution in Bayesian 
analysis system. In this problem, we then use MCMC method to obtain the posterior distribution of $\beta$ parameters from normal prior. Here `MCMClogit` function in `MCMCpack` package could be a good choice for us.

This function runs 11000 default iterations, starting with mean 0 and precision matrix of $0.01\textbf{I}$ under a multivariate normal prior. Discarding first 1000 iterarions, we save the big c(10000,12) posterior matrix into
an object `res_mcmc`.

```{r}
## Bayesian logistic
res_mcmc = MCMClogit(ytrain~Xtrain[,-1], b0 = 0, B0 = 0.01)
beta2_sd = sapply(1:(dim(res_mcmc)[2]), function(i) {sd(res_mcmc[,i])})
beta2_mean = colMeans(res_mcmc)
p2 = 2*(1-pnorm(abs(beta2_mean/beta2_sd)))
prob2 = rowMeans(1/(1+exp(-Xtest %*% t(res_mcmc))))
predtest2 = 1*(prob2>=0.5)
pred2 = mean(ytest==predtest2)
table(ytest, predtest2)
```

Bayesian logistic regression with LASSO prior is a more complex hierarchical model compared with previous
ones. We need to sample from the joint posterior formula of $\beta,\sigma^2$, and $\lambda$ is a fixed but unknow paramter that we need to eatimate first. So we first use cross-validation to select the optimal $\lambda$ which may minimize cross-validation error. By specifying $\lambda$, we fit a hierarchical Bayesian model to make references on test data. It is usually a little hard or tedious to realize the sampling process without cojugate priors. In terms of our knowledge, we could use another powerful tool Rstan to do Hamilton Monte Carlo sampling. See Appendix for Rstan script. we could also use a function contributed by other R users called `EBglmnet` to acheive the same objective, which is much concise. First, we use `cv.EBglmnet` function to select the optimal model. In last step using `EBglmnet` to sample from the established model. See references below to get more information about this fucntion.

```{r}
## Bayesian logistic regression model with LASSO prior: cv used to select model 
set.seed(3)
res_lassocv = cv.EBglmnet(Xtrain[,-1], ytrain, family="binomial", prior="lasso")
beta_lasso=c()
lamb_v=c(seq(1e-6,1e-4,by=1e-6),seq(1e-4,1e-2,by=1e-4),seq(0.01,0.1,by=1e-3),seq(0.1,0.5,by=0.01),seq(0.5,1,by=0.1))
for (lamb in lamb_v){
    res_lasso_i = EBglmnet(Xtrain[,-1], ytrain, family="binomial", prior="lasso", hyperparameters = lamb)
    beta_lasso_i = c(res_lasso_i$Intercept, rep(0,11))
    for(i in 1:nrow(res_lasso_i$fit)){
      beta_lasso_i[res_lasso_i$fit[i,1]+1] = res_lasso_i$fit[i,3]}
    beta_lasso=rbind(beta_lasso,beta_lasso_i)
}
```

```{r}
library(tidyverse)
names(beta_lasso)=paste0(rep('beta',12),1:12)
beta_lasso_plot=as.data.frame(cbind(lambda=lamb_v,beta_lasso))%>%
  gather(key="beta",value="value",-lambda)
ggplot(beta_lasso_plot, aes(x=lambda, y=value, group=beta)) +
  geom_line(aes(color=beta),size=1)+
  geom_point(aes(color=beta))+
  scale_color_brewer(palette="Paired")+
  ylim(-5,3)+
  geom_vline(xintercept =8.201102e-05, color = "#999999", size=1)+
  theme_minimal()+
  labs(x='lambda',y='parameters of beta')
```

From the result of `cv.EBglmnet`, we could obtain an optimal $\lambda$, which is 8.201102e-05. Fit model again using training data set,
we could get the hierachical model.

```{r}
res_lasso = EBglmnet(Xtrain[,-1], ytrain, family="binomial", prior="lasso", hyperparameters = 8.201102e-05)
beta3 = c(res_lasso$Intercept, rep(0,11))
for(i in 1:nrow(res_lasso$fit)){
  beta3[res_lasso$fit[i,1]+1] = res_lasso$fit[i,3]
}
pred3 = 1*(1/(1+exp(-Xtest %*% beta3))>=0.5)
mean(ytest==pred3)
table(ytest, pred3)
```

### to be continue



```{r}
# Histogram 
par(mfrow=c(3,4))
for(i in 1:12){
  hist(res_mcmc[,i], 
       main=sprintf("Posterior of beta %s", name[i]), 
       xlab=sprintf("%s", name[i]))
  abline(v=beta1[i], col="red")
}
```

```{r}
# CI plot
ci2 = data.frame(summary(res_mcmc)$quantiles, name=name, mean=beta2_mean)
colnames(ci2) = c("lower","quartile1","median","quartile3","upper","name","mean")
ci2[-1,] %>% # if exclude intercept
  ggplot( aes(x=name, y=mean) ) + 
  geom_point() +
  geom_point( aes(x=name, y=beta1[-1]), col="blue" ) +
  geom_errorbar( aes(ymin=lower, ymax=upper) ) +
  coord_flip() +
  theme_bw() + 
  ylab("beta") +
  xlab("")
ci2 %>% # if include intercept
  ggplot( aes(x=name, y=mean) ) + 
  geom_point() +
  geom_point( aes(x=name, y=beta1), col="blue" ) +
  geom_errorbar( aes(ymin=lower, ymax=upper) ) +
  coord_flip() +
  theme_bw() + 
  ylab("beta") +
  xlab("")
```

```{r}
# Correlation of beta
crosscorr.plot(res_mcmc)
```

```{r}
# p-values
p = data.frame(p1=p1, p2=p2)
knitr::kable(p, align="c")
```

## 4 Inference

### 4.1 Logistic Regression

### 4.2 Bayesian Logistic Regression with Normal prior

### 4.3 Bayesian Logistic Regression with NE prior


## 5 Prediction

### 5.1 Logistic Regression

### 5.2 Bayesian Logistic Regression with Normal prior

### 5.3 Bayesian Logistic Regression with NE prior

## 6 Discussion

## 7 Conclusion


## 8 Future Work

## 9 References

Wei, R., and Ghosal, S. (2017). Contraction properties of shrinkage priors in logistic regression, Preprint at http://www4.stat.ncsu.edu/~ghoshal/papers.

Genkin, A., Lewis, D. and Madigan, D. (2007). Large-scale Bayesian logistic regression for text categorization, \textit{Technometrics} \textbf{49}(3): 291–304.

Kapat, P., and Wang K. (2006). Classification Using Bayesian Logistic Regression: Diabetes in Pima Indian Women Example. Ohio State University, OH. https://www.asc.ohio-state.edu/goel.1/STAT825/PROJECTS/KapatWang_Team4Report.pdf

Anhui Huang and Dianting Liu (2016). EBglmnet: Empirical Bayesian Lasso and Elastic Net Methods for Generalized Linear Models. R package version 4.1. https://CRAN.R-project.org/package=EBglmnet

Andrew D. Martin, Kevin M. Quinn, Jong Hee Park (2011). MCMCpack: Markov Chain Monte Carlo in R. Journal of Statistical Software. 42(9): 1-21. URL http://www.jstatsoft.org/v42/i09/.

Li, L & Yao, W. (2017). Fully Bayesian logistic regression with hyper-LASSO priors for high-dimensional feature selection. Statistics 88, 1-25.



