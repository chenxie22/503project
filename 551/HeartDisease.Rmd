---
title: "551 Project, Team 10"
author: "Chen Xie, Xinye Jiang, Xun Wang"
date: "2019/4/13"
output:
  pdf_document: 
    fig_caption: yes
    keep_tex: yes
header-includes: \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,message=FALSE,warning=FALSE)
```

## 1. Introduction

Logistic regression is a very famous and widely applied technique in supervised learning. It is usually used to perform predictive analysis when the response variable is binary. 

Logistic regression can also be approached by Bayesian modeling. In general, Bayesian analysis is more flexible, and it is proved to be superior for small samples. For Bayesian modeling, it can incorporate prior information. For example, if we only want the most effective factors in the model, we can use some shrinkage priors to implement variable selection. 

In practice, predicting a binary response can be an application of the standard logistic regression as well as Bayesian approach. In this report, we delve into a data set about heart disease of 303 patients collected between May 1981 and September 1984 at the Cleveland Clinic in Cleveland, Ohio. The objective is to predict if the patients have heart disease based on 13 independent variables, such as age, sex, chest pain type, etc. In this process, we will also explore the different effects of specific predictors to response variable in different models. In addition, we will also compare among different logistic regression models.

## 2. Data Exploration

The dataset `heart.csv` is from UCI Machine Learning Repository. It has 303 observations and 14 variables in total. Every row is associated with a patient. The response is `target`, i.e. whether the angiographic result is present or absent of a >50% diameter narrowing (presence=1, absence=0). Because data in this dataset are all tested in a noninvasive way and are interpreted without knowledge of other test data nor historical results, there isn't any work-up bias, i.e. verification bias, during the data collection process. See more information about response and predictors in data description table below.

```{r}
## Libraries:
library(coda); 
library(ggplot2);library(gbm);library(GGally)
library(dplyr);library(tidyr)
library(knitr);library(kableExtra)
library(reshape2)
## Read Data:
h = read.csv("heart.csv")
h$sex=as.factor(h$sex)
h$cp=as.factor(h$cp)
h$fbs = as.factor(h$fbs)
h$restecg = as.factor(h$restecg)
h$exang = as.factor(h$exang)
h$slope = as.factor(h$slope)
h$ca = as.factor(h$ca)
h$thal = as.factor(h$thal)
## The response variable
#h$target = as.factor(h$target)
names(h)=c('age',names(h)[-1])
```
 
 
```{r,out.height='80%',out.width='80%'}
tbl0=tibble('Variables'=c('target','age','sex','cp','trestbps','chol','fbs',
                          'restecg','thelach','exang','oldpeak','slope','ca','thal'),
            'Type'=c('Binary','Continous','Binary','Categorical','Continous','Continous','Binary',
                     'Binary','Continous','Binary','Continous','Ordinal','Ordinal','Categorical'),
            'Collection'=c('Dependent Variable','Clinical Variable','Clinical Variable',
                           'Clinical Variable','Clinical Variable','Routine test',
                           'Routine test','Routine test','Noninvasive test','Noninvasive test',
                           'Noninvasive test','Noninvasive test','Noninvasive test',
                           'Noninvasive test'),
            'Description'=c('angiographic result of the presence or absence of a >50% diameter narrowing; presence = 1; absence = 0.','age in years','1=male; 0=female','chest pain type; 0=typical angina; 1=atypical angina; 2=non-anginal; 3=asymptomatic','systolic/resting blood pressure (in mm Hg on admission to the hospital)','serum cholestoral in mg/dl','(fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)',"resting electrocardiographic results; 0=normal; 1=having ST-T wave abnormality; 2=showing probable or definite left ventricular hypertrophy by Estes' criteria",'maximum heart rate achieved','exercise induced angina (1 = yes; 0 = no)','ST depression induced by exercise relative to rest','the slope of the peak exercise ST segment; 0=upsloping; 1=flat; 2=downsloping','number of major vessels (0-3) colored by flourosopy','exercise thallium scintigraphic defects; 3=normal; 6=fixed defect; 7=reversable defect'))
tbl0 %>%
  kable("latex", booktabs = T,
        caption = 'Data Description') %>%
  kable_styling(latex_options = c("striped", "hold_position","scale_down"))%>%
  column_spec(4,width='32em')
```

### 2.1 Numeric summary
Because our dataset is very clean, there isn't any missing value need to deal with. We simply show numeric summary of continous and categorical variables separately in two tables below. From the tables, we could see that there are 5 continous independent variables and 8 categorical ones.

```{r}
## Generate Summary table for continuous variables:
do.call(cbind, lapply(h[,c(1,4,5,8,10)],summary)) %>%
  kable("latex", booktabs = T,
        caption='Summary of Continous Variables') %>%
  kable_styling(latex_options = c("striped", "hold_position"))
## Genrate Summary table for categorical variables:
tbl1<-tibble('target'=c('0: 138','1: 165','','',''),
             'sex'=c('female: 96','male: 207','','',''), 
             'cp'=c('0: 143', '1: 50','2: 87','3: 23',''), 
             'fbs'=c('0: 258', '1: 45 ','','',''),
             'restecg'=c('0: 147', '1: 152','2: 4','',''),
             'exang'=c('0: 204', '1: 99','','',''),
             'slope'=c('0: 21','1: 140', '2: 142','',''),
             'ca'=c('0: 175','1: 65','2: 38','3: 20','4: 5'),
             'thal'=c('0: 2','1: 18','2: 166','3: 117',''))
tbl1 %>%
  kable("latex", booktabs = T,
        caption = 'Summary of Categorical Variables') %>%
  kable_styling(latex_options = c("striped", "hold_position")) 
```

### 2.2 Visualization

In the next step, let's have a direct look at heart disease dataset through visualization. Here is the pairwise scatterplots and histograms of continous varibales. These variables `age`,`trestbps`,`chol`,`thelach`,`oldpeak` display weak correlations between each of them.

```{r,out.height='70%',out.width='70%',fig.align='center',fig.cap='Pairwise scatterplots and histogram of continous variables.',fig.pos='H'}
## Pair plot including scatterplot, correation, histograms of continuous variables:
#png("pairs.png")
h_pairs=ggpairs(h[c(1,4,5,8,10)],axisLabels = "none",
        upper = list(continuous = "points", combo = "dot"),
        lower = list(continuous = "cor", combo = "dot"),
        diag = list(continuous = "barDiag")) + 
  theme_bw()
h_pairs
#dev.off()
```

In addition, we also explore boxplots of continous variables against target. It is noticed that median ages for both targets response are above 50 years old.

```{r,out.height='70%',out.width='70%',fig.align='center',fig.cap='Boxplots of continous variables.',fig.pos='H'}
## Generate boxplots of continuous variables by target(response):
p1=ggplot(h, aes(x=target,y=age,group=target)) +
  geom_boxplot(outlier.colour="black", outlier.shape=10,outlier.size=2, notch=FALSE,color=c("#E69F00", "#56B4E9"))
p2=ggplot(h, aes(y=trestbps,x=target,group=target)) +
  geom_boxplot(outlier.colour="black", outlier.shape=10,outlier.size=2, notch=FALSE,color=c("#E69F00", "#56B4E9"))
p3=ggplot(h, aes(y=chol,x=target,group=target)) +
  geom_boxplot(outlier.colour="black", outlier.shape=10,outlier.size=2, notch=FALSE,color=c("#E69F00", "#56B4E9"))
p4=ggplot(h, aes(y=thalach,x=target,group=target)) +
  geom_boxplot(outlier.colour="black", outlier.shape=10,outlier.size=2, notch=FALSE,color=c("#E69F00", "#56B4E9"))
p5=ggplot(h, aes(y=oldpeak,x=target,group=target)) +
  geom_boxplot(outlier.colour="black", outlier.shape=10,outlier.size=2, notch=FALSE,color=c("#E69F00", "#56B4E9"))
#png("boxplot.png")
grid.arrange(p1, p2, p3,p4,p5, nrow = 1)
#dev.off()
```

We also have a quick look at the bar charts of categorical variables against response variable. As we could see, response `target` is relatively balanced, while number of male (sex=1) is far more than the number of female.

```{r,out.height='70%',out.width='80%',fig.align='center',fig.cap='Bar charts of categorical variables.',fig.pos='H'}
## Barcharst of categorical variables by the response varaibles(target):
bp1<-ggplot(data=as.data.frame(table(h$target,h$sex)), aes(x=Var2, y=Freq, fill=Var1)) +
  geom_bar(stat="identity", position=position_dodge())+
  geom_text(aes(label=Freq), vjust=1.6, color="black",
            position = position_dodge(0.9), size=2.5)+
  scale_fill_brewer(palette="Paired")+
  labs(x='sex', y='Frequency',fill='target',title='Barchart of sex')+
  theme_minimal()
bp2<-ggplot(data=as.data.frame(table(h$target,h$cp)), aes(x=Var2, y=Freq, fill=Var1)) +
  geom_bar(stat="identity", position=position_dodge())+
  geom_text(aes(label=Freq), vjust=1.6, color="black",
            position = position_dodge(0.9), size=2.5)+
  scale_fill_brewer(palette="Paired")+
  labs(x='cp', y='Frequency',fill='target',title='Barchart of cp')+
  theme_minimal()
bp3<-ggplot(data=as.data.frame(table(h$target,h$fbs)), aes(x=Var2, y=Freq, fill=Var1)) +
  geom_bar(stat="identity", position=position_dodge())+
  geom_text(aes(label=Freq), vjust=1.6, color="black",
            position = position_dodge(0.9), size=2.5)+
  scale_fill_brewer(palette="Paired")+
  labs(x='fbs', y='Frequency',fill='target',title='Barchart of fbs')+
  theme_minimal()
bp4<-ggplot(data=as.data.frame(table(h$target,h$restecg)), aes(x=Var2, y=Freq, fill=Var1)) +
  geom_bar(stat="identity", position=position_dodge())+
  geom_text(aes(label=Freq), vjust=1.6, color="black",
            position = position_dodge(0.9), size=2.5)+
  scale_fill_brewer(palette="Paired")+
  labs(x='restecg', y='Frequency',fill='target',title='Barchart of restecg')+
  theme_minimal()
bp5<-ggplot(data=as.data.frame(table(h$target,h$exang)), aes(x=Var2, y=Freq, fill=Var1)) +
  geom_bar(stat="identity", position=position_dodge())+
  geom_text(aes(label=Freq), vjust=1.6, color="black",
            position = position_dodge(0.9), size=2.5)+
  scale_fill_brewer(palette="Paired")+
  labs(x='exang', y='Frequency',fill='target',title='Barchart of exang')+
  theme_minimal()
bp6<-ggplot(data=as.data.frame(table(h$target,h$slope)), aes(x=Var2, y=Freq, fill=Var1)) +
  geom_bar(stat="identity", position=position_dodge())+
  geom_text(aes(label=Freq), vjust=1.6, color="black",
            position = position_dodge(0.9), size=2.5)+
  scale_fill_brewer(palette="Paired")+
  labs(x='slope', y='Frequency',fill='target',title='Barchart of slope')+
  theme_minimal()
bp7<-ggplot(data=as.data.frame(table(h$target,h$ca)), aes(x=Var2, y=Freq, fill=Var1)) +
  geom_bar(stat="identity", position=position_dodge())+
  geom_text(aes(label=Freq), vjust=1.6, color="black",
            position = position_dodge(0.9), size=2.5)+
  scale_fill_brewer(palette="Paired")+
  labs(x='ca', y='Frequency',fill='target',title='Barchart of ca')+
  theme_minimal()
bp8<-ggplot(data=as.data.frame(table(h$target,h$thal)), aes(x=Var2, y=Freq, fill=Var1)) +
  geom_bar(stat="identity", position=position_dodge())+
  geom_text(aes(label=Freq), vjust=1.6, color="black",
            position = position_dodge(0.9), size=2.5)+
  scale_fill_brewer(palette="Paired")+
  labs(x='thal', y='Frequency',fill='target',title='Barchart of thal')+
  theme_minimal()
bp9<-ggplot(data=as.data.frame(table(h$target)), aes(x=Var1, y=Freq,fill=Var1)) +
  geom_bar(stat="identity", position=position_dodge())+
  geom_text(aes(label=Freq), vjust=1.6, color="black",
            position = position_dodge(0.9), size=2.5)+
  scale_fill_brewer(palette="Paired")+
  labs(x='target', y='Frequency',fill='target',title='Barchart of target')+
  theme_minimal()
#png("barchart.png")
grid.arrange(bp9,bp1,bp2,bp3,bp4,bp5,bp6,bp7,bp8, nrow = 3)
#dev.off()
#png("response.png")
#ggplot(data=as.data.frame(table(h$target)), aes(x=Var1, y=Freq,fill=Var1)) +
  #geom_bar(stat="identity", position=position_dodge())+
  #geom_text(aes(label=Freq), vjust=1.6, color="black",
            #position = position_dodge(0.9), size=5)+
  #scale_fill_brewer(palette="Paired")+
  #labs(x='target', y='Frequency',fill='target',title='Barchart of target')+
  #theme_minimal()
#dev.off()
```

### 2.3 Data Preparation

Before we perform logistic regression to dataset, we need to prepare our data first. First, we turn categorical variables into dummy variables at one step by using function `model.matrix`. The first column of result matrix is the intercept 1. As a result, the number of independent variables grow from 13 to 23.

Inaddition, we randomly split the whole dataset into training and test part at a rate around 8:2. Our training datset has 273 observations and test dataset has 30 observations.
```{r}
## Generate dummy variables for categorical data:
H = model.matrix(target~., data=h)
## Randomly split training and testing test:
set.seed(3)
ind = sample(1:303, 273, replace=F)
Xtrain = H[ind,]
Xtest = H[-ind,]
ytrain = h[ind,14]
ytest = h[-ind,14]
```

## 3. Logistic Regression

In this part, we will briefly introduce three logistic regression models, including standard one and Bayesian models. There are several advantages of logistic regression against other methods.  First, it can be interpreted. It helps to explain the relationship between the predictors and response variable. The logistic regression can also handle mixed types of exploratory variables. Most importantly, we have a binary response in this problem. So logistic regression models are most appropriate methods.

### 3.1 Logistic Regression

The basic assumption of standard logistic regression model is that observations $y_1, ..., y_n$ are independent and following binomial distribution (1, $p_i$), where $p_i$ is the probability of $y_i=1$. We could obtain point estimate of parameters $\beta$, which maximum $\Pi_{i=1}^n P(Y_i=y_i)$ using log-maximum likelihood approach. The statistical description is below:

\[y_1,...,y_n, \textup{ are independent}\]
\[y_i\sim \textup{ Binomial }(1, p_i)\]
\[\log(\frac{p_i}{1-p_i})=x_i^T\beta+\beta_0\]
\[p(y_i;\beta,\beta_0)=(p_i)^{y_i}(1-p_i)^{1-y_i}\]
\[p(y_1,...,y_n;\beta,\beta_0)=\prod_{i=1}^n(p_i)^{y_i}(1-p_i)^{1-y_i}\]

Among them, $x_i$ is the i-th row of observation and $\beta,\beta_0$ are parameters.

Using `glm` function, we could easily obtain estimates of $\beta$ parameters. The table 3 also provides us with other summary statistics of the result. From p.value column, we could observe that only 6 out of 23 predictors are significant in our model. If we adjust threshold to $\alpha=0.1$, there are still only 9 significant predictors. That's the reason why we want to use sparse solutions.

```{r}
## Fit original logistic regression:
res_original = glm(ytrain~., data=as.data.frame(Xtrain[,-1]), family=binomial(link="logit"))
## Inference: 
### p-values
p1 = summary(res_original)$coefficients[,4]
### estimates fo beta 
beta1 = res_original$coefficients
### estimates with 95% confidence interval for every beta
name = names(beta1)
ci1 = data.frame(lower=beta1, upper= beta1, name=name, mean=beta1, prior="None")
## Formatted Table of summary of the logistic regression 
## including estimate, se,z-value,p-value
coef2=cbind(variable=c('exang1','oldpeak','slope1','slope2','ca1','ca2','ca3','ca4','thal1','thal2','thal3',''),
            rbind( signif(coef(summary(res_original))[13:23,],5),c('','','','')))
data.frame(cbind(signif(coef(summary(res_original))[1:12,],5),coef2)) %>%
  kable("latex", booktabs = T,
        caption = 'Summary of Logistic Regression', digits= 5,
        col.names = c('Estimate','Std.Error','z.value','p.value',
                      'Variable','Estimate','Std.Error','z.value','p.value')) %>%
  kable_styling(latex_options = c("striped", "hold_position",'scale_down'))
```


### 3.2 Bayesian Logistic Regression with N-IG prior

In the standard logistic regression above, we treat $\beta$ as a column of unknown but fixed parameters. Actually, $\beta,\beta_0$ could be seen as a vector of random variables from the prospective of Bayesian analysis. In this way, we could combine data (model) and prior we build up to obtain not only a point estimate, but also posterior distribution of parameters. In addition to basic assumptions of standard logistic regression, bayesian logistic regression method with N-IG prior assumes that $\beta,\beta_0$ follows a normal prior with variance following an Inverse-Gamma distribution. Other parameters are assumed to be flat because we don't have much information at first.


\[y_1,...,y_n, \textup{ are independent}\]
\[\textup{Likelihood: } y_i\sim \textup{ Binomial }(1, p_i)\]
\[\textup{Parameters: }\log(\frac{p_i}{1-p_i})=x_i^T\beta\]
\[\textup{Prior: } \beta_i\sim N(\mu_i,\sigma_i^2)\]
\[\textup{Hyper prior: } \sigma_i^2\sim \textup{Inv-Gamma}(a,b)\]
\[\mu_i,a,b\textup{are assumed to have flat distribution.}\]

Using a powerful tool in R: Rstan, we could implement Monte Carlo Markov Chain algorithm to our model and obtain posterior samples of parameters $\beta$. In addition, 95% credible interval are naturally to be computed based on quantiles of posterior samples. The plot below shows the comparision of parameters obtained by both standard logistic regression and bayesian logistic. The red lines represent $\beta$ estimated from standard logistic regression and green lines represent 95% credible interval produced by posterior samples of parameters, while blue
lines are 0.

```{r eval=FALSE}
## Using rstan to draw MCMC posterior samples
library(rstan)
set.seed(551)
dat = list(k=dim(Xtrain)[2], n=dim(Xtrain)[1], x=as.matrix(Xtrain), y=ytrain)
chain2 = stan(file="STATS551_project1.stan", data = dat, iter=2000)
```

```{r eval=FALSE}
## Extract beta (interested parameters)
params2 = extract(chain2)
beta2 = params2[["beta"]]
```

```{r}
params2=readRDS('stan2.rds')
beta2 = params2[["beta"]]
```

```{r eval=FALSE}
## 95% credible interval based on quantiles of posterior sample
## also get point estimate by mean of posterior sample
ci2 = data.frame(t(apply(beta2, 2, function(x) quantile(x, probs=c(0.025, 0.975)))), name=name, mean=colMeans(beta2), prior="N-IG")
colnames(ci2) = c("lower","upper","name","mean","prior")
## Histograms of posterior samples of beta
## also plot credible interval 
## also check if 0 in the interval
## also compare point estimate of logistic regression to the posterior samples
par(mfrow=c(4,6))
for(i in 1:(dim(beta2)[2])){
  # Histograms
  hist(beta2[,i], 
       main=sprintf("Posterior of beta %s", name[i]), 
       xlab=sprintf("%s", name[i]), xlim=c(min(beta1[i],0,beta2[,i]), max(beta1[i],0,beta2[,i])))
  # point estimate of logistic regression
  abline(v=beta1[i], col="red")
  # line beta=0
  abline(v=0, col="blue")
  # 95% credible interval 
  abline(v=ci2[i,1], col="green", lty=2)
  abline(v=ci2[i,2], col="green", lty=2)
}
```


```{r,fig.align='center',fig.cap='Estimate of parameters beta.',fig.pos='H'}
knitr::include_graphics(c("beta.png"))
```

Most red lines of parameters from standard logistic regression fall into the credible intervals of posterior distribution, meaning most variables produce similar effects in both models. In contrast, absolute value of parameters of variables `intercept`, `thal1`, `thal2` and `thal3` in bayesian logistic regression are much smaller compared to standard model. That may because bayesian logsitic regression could take full use of information from data and adjust the effects of these variables properly.

### 3.3 Bayesian Logistic Regression with NEG prior

As we could see from the summary table of standard logistic regression, most variables actually do not produce significant effects to the response. In order to figure out the most important predictors to the `target` response and to improve the interpretation of our result, we choose to use bayesian logistic models with shrinkage priors. There are many priors which are proved to have shrinkage effects to parameters, for example, Cauchy prior, Laplace prior and horseshoe prior, we choose Normal-Exponential-Gamma prior for consistency with two previous models.

\[y_1,...,y_n, \textup{ are independent}\]
\[\textup{Likelihood: } y_i\sim \textup{ Binomial }(1, p_i)\]
\[\textup{Parameters: }\log(\frac{p_i}{1-p_i})=x_i^T\beta\]
\[\textup{prior: } \beta_i\sim N(0,\sigma_i^2)\]
\[\textup{Hyper Prior: } P(\sigma_i^2) \sim \textup{Exponential }(\lambda)\]
\[\lambda\sim \textup{Gamma }(a_0,b_0),\textup{ where }a_0,b_0 \textup{ are two fixed parameters.}\]

The reason for NEG prior to have a shrinkage influence to $\beta,\beta_0$ is that the exponential distribution of variance lays a great mass of probabilities around 0. As a result, initial $\beta,\beta_0$ have a large probability to gather around 0. 

Here, another large problem for this model is how to choose hyper parameters $a_0,b_0$. We then decide to use a method similar to model checking. First, we propose a pair of possible $a_0,b_0$. Second, based on our model, we simulate a $\lambda$ and a matrix of ‘Fake Data’, which is generated from normal distribution $N(0,1)$. Then we could get a vector of generated responses following the process of our model. In the next step, repeat the former steps for 1000 times and obtain a distribution of a summarized statistic (e.g., mean) of every group of generated response. Finally, comparing the distribution of `mean` summary statistic to the one of true data, we choose reasonable values of $a_0,b_0$.

```{r,out.height='50%',out.width='50%',fig.align='center',fig.cap='Histogram of the posterior distribution of lambda',fig.pos='H'}
## a,b: a proposal of hyperparamters
## k: number of predictors, n : number of observations
hyperprior_para_seletion=function(a,b,n,k){
  # Initialize
  mean_data=c()
  set.seed(3)
  # Fake indenpendent data
  X=matrix(rnorm(n*k),nrow=n,ncol=k) 
  # Iteration=1000
  for (i in 1:1000){
    # simulate lambda, variances, beta
    lamb=rgamma(1,shape=a,rate=b)
    sig2=rexp(k,rate=lamb)
    betai=rnorm(k,mean=0,sd=sqrt(sig2))
    # simulate intercept
    alph=rnorm(1,mean=0,sd=2)
    # compute probabilities
    p=exp(alph+X%*%betai)/(1+exp(alph+X%*%betai))
    # simulate Fake y's (response variable)
    y=rbinom(n,size=1,prob=p)
    # compute summary statistic (mean)
    mean_data=c(mean_data,mean(y))
    
    # other options for summarized statistic: e.g., variance
    ##var_data=c(var_data,var(y))
  }
  # return a distribution of summary statistic of fake data for a proposal of (a,b)
  return(mean_data[!is.na(mean_data)])
}
## Compare to the mean of true data and plot
#layout(mat=matrix(c(1,2,3,4)))
par(mfrow=c(2,2))
hist(hyperprior_para_seletion(a=0.5,b=0.5,n=273,k=22),xlab='mean of generated y',main='alpha=0.5,beta=0.5')
abline(v=mean(ytrain),col='red')
hist(hyperprior_para_seletion(a=5,b=5,n=273,k=22),xlab='mean of generated y',main='alpha=5,beta=5')
abline(v=mean(ytrain),col='red')
hist(hyperprior_para_seletion(a=100,b=1,n=273,k=22),xlab='mean of generated y',main='alpha=100,beta=1')
abline(v=mean(ytrain),col='red')
hist(hyperprior_para_seletion(a=1,b=100,n=273,k=22),xlab='mean of generated y',main='alpha=1,beta=100')
abline(v=mean(ytrain),col='red')

```

In this example, we try several pairs of $(a_0, b_0)$ and results are plotted above. Though both $a_0=0.5,b_0=0.5$ and $a_0=5,b_0=5$ are appropriate, we finally choose $a_0=5,b_0=5$ as its best performance.

Also implementing this model in Rstan, we obtain both the posterior distributions of $\beta,\beta_0$ and $\lambda$. From the posterior distribution of $\lambda$, bayesian method show its power that posterior distribution of $\lambda$ is more centered around 0.8 compared to prior. And we will discuss about the differences between posterior distribution of parameters $\beta,\beta_0$ of bayesian logistic regression with NEG prior to parameters from other two models in next section.

```{r eval=FALSE}
set.seed(551)
## Using rstan to implement MCMC sampling draw posterior sample 
dat = list(k=dim(Xtrain)[2], n=dim(Xtrain)[1], x=as.matrix(Xtrain), y=ytrain)
chain3 = stan(file="STATS551_project2.stan", data = dat, iter=2000)
```

```{r eval=FALSE}
## Extract beta and lambda (interested parameters) 
params3 = extract(chain3)
beta3 = params3[["beta"]]
lambda3=params3[['lambda']]
```

```{r}
params3=readRDS('stan3.rds')
beta3 = params3[["beta"]]
lambda3=params3[['lambda']]
```

```{r}
## 95% credible interval based on quantiles of posterior sample
## also get point estimate by mean of posterior sample for NEG prior
ci3 = data.frame(t(apply(beta3, 2, function(x) quantile(x, probs=c(0.025, 0.975)))), name=name, mean=colMeans(beta3), prior="N-E-G")
colnames(ci3) = c("lower","upper","name","mean","prior")
```

```{r,eval=FALSE}
## Plot of prior and posterior of lambda
## png('lambda.png')
hist(lambda3,freq=FALSE,ylim=c(0,1.4))
lines(density(lambda3),xlab="lambda")
lines(seq(0,2.7,0.1), dgamma(seq(0,2.7,0.1), 5, 5), col="red")
legend(2, 1.2, c("prior",'posterior'), col=c("red",'black'), lty=c(1,1))
## dev.off()
```

```{r,out.height='60%',out.width='60%',fig.align='center',fig.cap='Histogram of the posterior distribution of lambda',fig.pos='H'}
knitr::include_graphics(c("lambda.png"))
```


## 4 Inference 
### 4.1 Confidence Interval

```{r eval=FALSE}
# Compare interval estimation and plot
ci = rbind(ci1,ci2,ci3)
## png('ci.png')
ci %>% # if include intercept
  ggplot( aes(x=name, y=mean, color=prior) ) + 
  geom_point( position = position_dodge(.5) )  +
  geom_errorbar( aes(ymin=lower, ymax=upper), position = position_dodge(.5) ) +
  scale_color_manual( values = c('red', 'orange', '#56B4E9')) +
  coord_flip() +
  theme_bw() + 
  ylab("beta") +
  xlab("")
## dev.off()
```

Below is the plot of point estimate and estimated confidence intervals of parameters $\beta,\beta_0$ from all three different models above. From the plot, we could observe that NEG prior does have strong shrinkage effects to our model that 0 is included in most of blue confidence intervals. We also notice that variables kept are exactly the siginificant variables in standard logistic regression. In addition, confidence intervals with NEG prior are narrowed down compared to orange confidence intervals, for example, for variables `slope1` and `intercept`, meaning more concentrated and precise estimates of parameters. In conclusion, bayesian logistic regression with NEG prior is a very efficient method combining model fitting with variable selection at the same time.


```{r,fig.align='center',fig.cap='Confidence interval and point estimate of beta',fig.pos='H'}
knitr::include_graphics(c("ci.png"))
```


### 4.2 Prediction

Based on standard logistic regression model, it is easy to make predictions using function `predict`. Thus we obtain the confusion matrix with 22 right predictions and 8 wrong predictions as below.

```{r}
## Prediction of Logistic Regression: 
predtest1 = predict(res_original, newdata=as.data.frame(Xtest[,-1]), type="response")
### Predictive Accuracy
###accuracy1 = mean(predtest1==ytest)
### log loss
logloss1=sum(-log(predtest1[ytest==1]))+sum(-log(1-predtest1[ytest==0]))
### Predictive Sensitivity
sensitivity1 = sum(predtest1*ytest)/sum(ytest)
```

```{r}
## Confusion matrix for prediction result
data.frame(v1=c('Actual: No','Actual: Yes'),v2=c(11, 4),v3=c(4, 11)) %>%
  kable("latex", booktabs = T,caption = 'Confusion matrix for logistic regression',align = 'r',
        col.names = c('','Predicted: No','Predicted: Yes'))%>%
  kable_styling(latex_options = c("striped", "hold_position"))
```

In comparision, if we want to use the fitted bayesian logistic regression models to make predictions, we should sample from posterior distributions of $\beta,\beta_0$, because posterior distribution has beed updated by  incorporating information from data and prior. First, we randomly sample many groups of $\beta$ from posterior distribution, and then randomly sample new data $y$ from samples of $\beta$, the same size of responses as test dataset for each group of 23 parameters. As a result, we could obtain many groups of predictions for test data.
For each group of predicted data, we could compare them with original test data.

In the next step, we are going to use two indeces `log-loss` and `sensitivity` to evaluate the prediction performances of three models.

```{r}
## Prediction of Bayesian Logistic Regression with N-IG prior:
set.seed(551)
### sample 2000 times from the posterior sample of beta
beta2_sample_ind = sample(1:4000, 5000, replace=T)
### calculate predicted probability based on the predictive sample
prob2 = 1/(1+exp(-as.matrix(Xtest) %*% t(beta2[beta2_sample_ind,])))
### predict the response variable
predtest2 = matrix(rbinom(5000*30, 1, prob=prob2), nrow=30)
### predictive accuracy
### accuracy2_dist = apply(predtest2, 2, function(x) mean(x==ytest))
### point estimation of accuracy2_dist
### accuracy2= mean(accuracy2_dist)
### log loss
logloss2_dist=sapply(1:5000,function(i){sum(-log(prob2[ytest==1,i]))+sum(-log(1-prob2[ytest==0,i]))})
logloss2=mean(logloss2_dist)
### predictive sensitivity
sensitivity2_dist = apply(predtest2, 2, function(x) sum(x*ytest))/sum(ytest)
sensitivity2=mean(sensitivity2_dist)
```

```{r}
set.seed(551)
## Prediction of Bayesian Logistic Regression with NEG prior
### Sampling from posterior sample of beta
beta3_sample_ind = sample(1:4000, 5000, replace=T)
prob3 = 1/(1+exp(-as.matrix(Xtest) %*% t(beta3[beta3_sample_ind,])))
### Generate predicted response
predtest3 = matrix(rbinom(5000*30, 1, prob=prob3), nrow=30)
### predictive accuracy
### accuracy3_dist = apply(predtest3, 2, function(x) mean(x==ytest))
### point estimation of accuracy2_dist
### accuracy3= mean(accuracy3_dist)
### log loss
logloss3_dist=sapply(1:5000,function(i){sum(-log(prob3[ytest==1,i]))+sum(-log(1-prob3[ytest==0,i]))})
logloss3=mean(logloss3_dist)
### predictive sensitivity
sensitivity3_dist = apply(predtest3, 2, function(x) sum(x*ytest))/sum(ytest)
sensitivity3=mean(sensitivity3_dist)
```


#### 4.2.1 Log-Loss

Log-loss or logarithmic loss is an metric used in classification problems to compare the prediction performanes of different models. The lower log-loss is, the better performance of the model has. Log-loss also has an requirement that we need the raw probabilities of $P(y=1|\beta,\beta_0)$ as well as the true response. For logistic regression, log-loss is defined as:

\[-log(y|p)=-ylog(p)-(1-y)log(1-p)\]
\[\textup{ where } y \textup{ is the target value 0 or 1, } p \textup{ is the probability } P(y=1|\beta,\beta_0).\]

Using this formula, we calculate the log-loss value of standard logistic regression and density distribution of log-loss of bayesian logistic regression models and show them into one plot. Average log-loss for bayesian methods are also computed and marked on the plot below. 

```{r,out.height='70%',out.width='70%',fig.align='center',fig.cap='Histogram of predictive log-loss',fig.pos='H'}
logloss23=melt(data.frame('N-IG'=logloss2_dist,'NEG'=logloss3_dist))
names(logloss23)=c('prior','value')
ggplot(logloss23,aes(x=value, fill=prior))+
  geom_vline(xintercept =logloss1, color = "#999999", size=1.2,text='Logistic')+
  geom_histogram(alpha=0.5,aes(y = ..density..))+
  xlim(0,30)+
  geom_vline(xintercept =logloss2, color = "#F8766D", size=1.2)+
  geom_vline(xintercept =logloss3, color = "#00BFC4", size=1.2)+
  xlab('log-loss')
```
`

It is showed that log-loss of bayesian models have a large improvement compared to logistic regression, while model with NEG prior is samller than model with N-IG prior in log-loss. In summary, bayesian logistic regression model with shrinkage prior performs best in prediction.

### 4.2.2 Sensitivity

Sensitivity is another index of great importance to heart disease detection. If a patient does have heart disease, it will be a big deal if doctors misdiagnose him not to have. Therefore, sensitivity is used to measure the true positive rate of our prediction. That is, the proportion of patients with heart diseases to be diagnosed correctly.

We also use a plot to illustrate the superiority of bayesian logistic regression model with NEG prior. See it below.
Logistic regression produces a sensitivity of 73.33% and bayesian model with N-IG prior shows 72.22% sensitivity, while bayesian model with NEG prior has an average sensitivity of 74.52%, higher than both of others.

```{r,out.height='70%',out.width='70%',fig.align='center',fig.cap='Histogram of predictive sensitivity',fig.pos='H'}
sensitivity23=melt(data.frame('N-IG'=sensitivity2_dist,'NEG'=sensitivity3_dist))
names(sensitivity23)=c('prior','value')
ggplot(sensitivity23,aes(x=value, fill=prior))+
  geom_vline(xintercept =sensitivity1, color = "#999999", size=1.2,text='Logistic')+
  geom_histogram(alpha=0.5,aes(y = ..density..))+
  geom_vline(xintercept =sensitivity2, color = "#F8766D", size=1.2)+
  geom_vline(xintercept =sensitivity3, color = "#00BFC4", size=1.2)+
  xlab('log-loss')
```


## 5. Conclusion & Future Work

In conlusion, for large dataset, logistic regression and bayesian inference may produce very similar results. But in general, bayesian modeling is more flexible and is better when we do not have many observations. In this case, we may also use sparse solution assumption for parmeters to shrink non-significant variables to 0 in our model. In this way, we could deal with high correlation between variables and reduce variable dimensionalty. Only factors which produce important effects to response could be kept. Then it is easier for us to intepreat models. 

For this dataset, bayesian logistic regression with NEG prior has the best prediction performances on both log-loss and sensitivity compared to standard logistic regression and bayesian logistic regression with N-IG prior. 

For future work, we may try more bayesian models with various priors and test their performances in prediction.


## 6. Reference
http://archive.ics.uci.edu/ml/datasets/heart+disease

Wei, R. & Ghosal, S. (2017). Contraction properties of shrinkage priors in logistic regression, Preprint at http://www4.stat.ncsu.edu/~ghoshal/papers.

Genkin, A., Lewis, D. & Madigan, D. (2007). Large-scale Bayesian logistic regression for text categorization. Technometrics, 49(3): 291–304.

Kapat, P. & Wang, K. (2006). Classification Using Bayesian Logistic Regression: Diabetes in Pima Indian Women Example. Ohio State University, OH. https://www.asc.ohio-state.edu/goel.1/STAT825/PROJECTS/KapatWang_Team4Report.pdf

Li, L & Yao, W. (2017). Fully Bayesian logistic regression with hyper-LASSO priors for high-dimensional feature selection. Statistics 88, 1-25.

Park, T. & Casella, G. (2008). The Bayesian LASSO, Journal of the American Statistical Association, 103: 482, 681-686.

Detrano, R., Janosi, A., Steinbrunn, W., Pfisterer, M., Schmid, J. J., Sandhu, S., Guppy, K., Lee, S. & Froelicher, V. (1989). International application of a new probability algorithm for the diagnosis of coronary artery disease. The American journal of cardiology, 64(5): 304-310. 

http://wiki.fast.ai/index.php/Log_Loss

\pagebreak

## Appendix
