---
title: "Stats 503 Project: Gesture"
author: "Chen Xie, Xun Wang, Xinye, Jiang"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE,warning = FALSE,fig.align='center')
```

## Data Exploration

```{r}
# Data Description
t1=data.frame(name=c("V65","V1-V8","V9-V16","V17-V24","V25-V32","V33-V40","V41-V48","V49-V56","V57-V64"),
              type=c("categorical",rep("continuous",8)),des=c("response",paste0("Sensor",1:8)))
cap="Data Description"
knitr::kable(t1,format='pandoc',caption=cap,align='c',
             col.names=c('Variables names','Type','Description'))
```

```{r}
# Load libraries and datasets
library(gplots); library(randomForest); library(ggplot2); library(gbm); library(foreign); library(nnet)
library(MASS); library(class); library(e1071); library(tree)
dat0 = read.csv("0.csv", header = F)
dat1 = read.csv("1.csv", header = F)
dat2 = read.csv("2.csv", header = F)
dat3 = read.csv("3.csv", header = F)
g = rbind(dat0, dat1, dat2, dat3)
g$V65 = as.factor(g$V65)
```

```{r}
heatmap.2(cor(g[,-1]), dendrogram='none', Rowv=FALSE, Colv=FALSE, trace='none')
```

```{r}
## boxplots
ggplots=list()
for (i in 1:64){
  p=ggplot() +
  geom_boxplot(aes(x=g[,65],y=g[,i],color=g[,65]),outlier.colour="black", outlier.shape=10,outlier.size=2, notch=FALSE)+
  labs(title=sprintf('Boxplot of sensor %s signal %s',  ifelse(i%%8!=0,i%/%8+1,i/8), ifelse(i%%8!=0,i%%8,8) ) , x='Gesture',y='Sensor')+
  scale_colour_discrete(name="Gesture")+
  theme_classic()
  ggplots[[i]]=p
}

```

## Variable Selection

```{r}
set.seed(77)
rf=randomForest(V65~.,data=g,mtry=8,importance=TRUE)
```

```{r}
varImpPlot(rf)
```


## Split data
```{r}
# Split the dataset into training and test sets
set.seed(77)
g_new=g[,c(seq(from=2,to=58,by=8),seq(from=7,to=63,by=8),65)]
train=sample(1:nrow(g_new),size=nrow(g_new)*0.8,replace=FALSE)
```

## Classification Methods

### Logistic Regression

```{r}
#min.model = multinom(V65~1, data=g[train,])
#biggest = multinom(V65~., data=g[train,])
#fwd1.model = step(min.model, scope=formula(biggest), direction="forward")
#summary(fwd1.model)
#pred_lr1 = predict(fwd1.model, g[-train,])
#mean(pred_lr1 == g[-train,65])
#fwd2.model = step(min.model, scope=formula(biggest), direction="forward", k=log(length(train)))
#summary(fwd2.model)
```

```{r}
set.seed(3)
lr_fit = multinom(V65~., data=g_new[train,])
lr_pred = predict(lr_fit, g_new[-train,])
rate_lr = mean(lr_pred == g_new[-train,'V65'])
table(lr_pred, g_new[-train,'V65'])
rate_lr
```

### LDA

```{r}
lda_fit = lda(V65~., data=g_new[train,])
lda_pred = predict(lda_fit, g_new[-train,])
rate_lda = mean(lda_pred$class == g_new[-train,'V65'])
table(lda_pred$class, g_new[-train,'V65'])
rate_lda
```

### QDA

```{r}
qda_fit = qda(V65~., data = g_new[train,])
qda_pred = predict(qda_fit, g_new[-train,])$class
table(qda_pred, g_new[-train,'V65'])
rate_qda = mean(qda_pred==g_new[-train,'V65'])
rate_qda
```


### KNN

```{r}
knn_cv_error = NULL
k_list = 1:20
set.seed(3)
knn.cv = function(k, t, nfolds=5) {
  n_tr = nrow(t)
  s = split(sample(n_tr), rep(1:nfolds, length=n_tr))
  cv_error = 0
  for(i in seq(nfolds)){
    p.cv = knn(t[-s[[i]],-17], t[s[[i]],-17], t[-s[[i]],17], k=k)
    cv_error = cv_error + mean(p.cv!=t[s[[i]],17])
  }
  cv_error = cv_error / nfolds
}
for(k in k_list) {
  knn_cv_error = c(knn_cv_error, knn.cv(k, g_new[train,]))
}
```

```{r, fig.cap=cap, fig.align = "center"}
cap = 'The 5-fold Cross Validation Errors for Each Choice of K.'
plot(k_list, knn_cv_error, type='l', ylim=c(0.13, 0.23),
     xlab='Number of Nearest Neighbors K', ylab='Cross Validation Error')
points(k_list[which.min(knn_cv_error)], min(knn_cv_error))
text(k_list[which.min(knn_cv_error)], min(knn_cv_error)-0.01, "3")
```

Choose K=3.

```{r}
set.seed(3)
knn_pred = knn(g_new[train,-17], g_new[-train,-17], g_new[train,17], k=3)
rate_knn = mean(knn_pred == g_new[-train,17])
table(knn_pred, g_new[-train,17])
rate_knn
```

### SVM
Validation to choose the optimal parameters
```{r}
set.seed(77)
valid=sample(train,size=floor(length(train)*0.2),replace=FALSE)
train2=setdiff(train,valid)
costv=c(0.01,0.1,1,10,100)
gammav=c(0.5,1,2,3,4)
validresult=cbind(expand.grid(costv,gammav),0)
for (i in 1:nrow(validresult)){
  svm_fit=svm(V65~.,data=g_new[train2,], kernel="radial", cost=validresult[i,1],gamma=validresult[i,2])
  svm_pred=predict(svm_fit,g_new[valid,])
  validresult[i,3]=mean(svm_pred==g_new[valid,"V65"])
}
```

```{r}
which.max(validresult[,3])
```

So, choose cost=1, gamma=0.5.

```{r}
svm_fit=svm(V65~.,data=g_new[train,], kernel="radial", cost=1,gamma=0.5)
svm_pred=predict(svm_fit,g_new[-train,])
rate_svm=mean(svm_pred==g_new[-train,"V65"])
rate_svm
```

### Classification Tree
```{r}
tree_fit=tree(V65~.,data=g_new[train,])
tree_pred=predict(tree_fit,g_new[-train,],type="class")
table(tree_pred,g_new$V65[-train])
rate_tree=mean(tree_pred==g_new$V65[-train])
rate_tree
```


### Random Forest
```{r}
set.seed(77)
rf_fit=randomForest(V65~.,data=g_new[train,],mtry=4,importance=TRUE)
rf_pred = predict(rf_fit,newdata=g_new[-train,])
table(rf_pred,g_new$V65[-train])
rate_rf=mean(rf_pred==g_new$V65[-train])
rate_rf
```


## Prediction Accuracy Comparison
```{r}
t2=data.frame(method=c("Logistic Regression","LDA","QDA","KNN","SVM","Classification Tree","Random Forest"),
              pred=c(rate_lr,rate_lda,rate_qda,rate_knn,rate_svm,rate_tree,rate_rf))
cap="Prediction Accuracy of several methods"
knitr::kable(t2,format='pandoc',caption=cap,align='c',
             col.names=c('Method','Prediction Accuracy'))
```




