---
title: "Stats 503 Project: Credit Default"
author: "Chen Xie, Xun Wang, Xinye Jiang"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE,warning = FALSE,fig.align='center',eval=FALSE)
```

## 1. Introduction 

## 2. Data Exploration

```{r}
## Read the Data
dat0 = read.csv("0.csv", header = F)
dat1 = read.csv("1.csv", header = F)
dat2 = read.csv("2.csv", header = F)
dat3 = read.csv("3.csv", header = F)
g = rbind(dat0, dat1, dat2, dat3)
g$V65=as.factor(g$V65)

```

```{r}
## Libraries
library(ggplot2);library(GGally);library(gbm);library(reshape2);library(gplots)
library(dplyr);library(e1071);library(tree);library(class);library(MASS);library(nnet)
library(randomForest);library(foreign);

## Summary table 1
t1=data.frame(name=c("V65","V1-V8","V9-V16","V17-V24","V25-V32","V33-V40","V41-V48","V49-V56","V57-V64"),
              type=c("categorical",rep("continuous",8)),des=c("Response; rock:0, scissors:1, paper:2, ok:3",paste0("Reading",1:8)))
cap="Data Description"
knitr::kable(t1,format='pandoc',caption=cap,align='l',
             col.names=c('Variables names','Type','Description'))
```




```{r, echo=FALSE,out.width="80%", out.height="80%",fig.cap="Barchart of Gestures",fig.show='hold',fig.align='center'}
## Barchart of response variable
png('response_barchart.png')
ggplot(as.data.frame(table(as.factor(g$V65))),aes(x=Var1,fill=Var1,y=Freq))+
  geom_bar(stat = "identity")+
  geom_text(aes(label=Freq,x=Var1), vjust=1.6, 
            color="white", size=3.5)+
  labs(title = "Frequency of Gestures", x = "Gestures", y = "Frequency") +
  scale_fill_discrete(name="Gesture")+
  theme_classic()
dev.off()
```


```{r}
## Scatterplots
png('scatterplot_1.png')
ggplot(g, aes(x=g[,15],y=g[,7], color=g[,65]))+
  geom_point()+
  scale_color_discrete(name="Gesture")+
  labs(x='reading 2 sensor 7 ',y='reading 1 sensor 7', title='Scatterplot of two readings for the same sensor')
dev.off()

png('scatterplot_2.png')
ggplot(g, aes(x=g[,2],y=g[,7], color=g[,65]))+
  geom_point()+
  scale_color_discrete(name="Gesture")+
  labs(x='reading 1 sensor 2 ',y='reading 1 sensor 7', title='Scatterplot of two sensor for the same reading')
dev.off()
```

```{r}
## Plots of Correlation
ggplot(data = melt(cor(g[,-65])), aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile(color = "white")+
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1,1), space = "Lab", name="Pearson\nCorrelation") +
  theme_minimal()+ 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1))+
  labs(x='',y='',title='Correlation (Overview)')

ggplot(data = melt(cor(g[,1:16])), aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile(color = "white")+
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1,1), space = "Lab", name="Pearson\nCorrelation") +
  theme_minimal()+ 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1))+
  geom_rect(aes(xmin = 1 - 0.5, xmax = 8 + 0.5, ymin = 1 - 0.5, ymax = 8 + 0.5),
            fill = "transparent", color = "grey", size = 1.5)+  
  geom_rect(aes(xmin = 9 - 0.5, xmax = 16 + 0.5, ymin = 9 - 0.5, ymax = 16 + 0.5),
            fill = "transparent", color = "grey", size = 1.5)+
  geom_rect(aes(xmin = 3 - 0.5, xmax = 3 + 0.5, ymin = 4 - 0.5, ymax = 4 + 0.5),
            fill = "transparent", color = "darkred", size = 1)+
  geom_rect(aes(xmin = 1 - 0.5, xmax = 1 + 0.5, ymin = 9 - 0.5, ymax = 9 + 0.5),
            fill = "transparent", color = "blue", size = 1)+
  geom_rect(aes(xmin = 1 - 0.5, xmax = 4 + 0.5, ymin = 1 - 0.5, ymax = 4 + 0.5),
            fill = "transparent", color = "orange", size = 0.5)+
  geom_rect(aes(xmin = 5 - 0.5, xmax = 8 + 0.5, ymin = 5 - 0.5, ymax = 8 + 0.5),
            fill = "transparent", color = "orange", size = 0.5)+
  labs(x='',y='',title='Correlation of First Two Readings')

sensors_order=c(seq(1,64,by=8),seq(2,64,by=8),seq(3,64,by=8),seq(4,64,by=8),seq(5,64,by=8),seq(6,64,by=8),seq(7,64,by=8),seq(8,64,by=8))

ggplot(data = melt(cor(g[,sensors_order])), aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile(color = "white")+
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1,1), space = "Lab", name="Pearson\nCorrelation") +
  theme_minimal()+ 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1))+
  labs(x='',y='',title='Correlation (In Sensors Order)')+
  geom_rect(aes(xmin = 1 - 0.5, xmax = 32 + 0.5, ymin = 1 - 0.5, ymax = 32 + 0.5),
            fill = "transparent", color = "lightblue", size = 0.5)+  
  geom_rect(aes(xmin = 33 - 0.5, xmax = 64 + 0.5, ymin =33 - 0.5, ymax = 64+ 0.5),
            fill = "transparent", color = "lightblue", size = 0.5)+
  geom_rect(aes(xmin = 1 - 0.5, xmax = 8 + 0.5, ymin =1 - 0.5, ymax = 8 + 0.5),
            fill = "transparent", color = "grey", size = .5)+
  geom_rect(aes(xmin = 1 - 0.5, xmax = 8 + 0.5, ymin =9- 0.5, ymax = 16 + 0.5),
            fill = "transparent", color = "orange", size = .5)
```


```{r,out.height='20%',out.width='20%'}
## Hitograms of reading 1 of sensor 1 and sensor 7
ggplots2=list()
for (i in c(1,7)){
  p=ggplot(g, aes(x=g[,i],color=V65)) +
  geom_density()+
  labs(title=sprintf('Density of reading %s sensor %s',  ifelse(i%%8!=0,i%/%8+1,i/8), ifelse(i%%8!=0,i%%8,8) ) , x='Gesture',y='Sensor')+
  scale_colour_discrete(name="Gesture")+
  theme_classic()
  ggplots2[[i]]=p
}

for (i in c(1,7)) {
  png(sprintf('r1sensor%s.png',i))
  print(ggplots2[[i]])
  dev.off()
}


```

```{r}
ggplot(g,aes(color=V65,x=V2,y=V7)) +
  geom_density_2d(mapping = NULL, data = NULL, stat = "density2d",
  position = "identity", lineend = "butt", linejoin = "round",
  linemitre = 10, na.rm = FALSE, show.legend = NA,
  inherit.aes = TRUE)+
  labs(title='Contour of Density of V2 and V7')+
  scale_colour_discrete(name="Gesture")+
  theme_classic()
```


## 3. Variable Selection

```{r}
## Perform Variable Selection
## by random forest and check the importance of variables
set.seed(77)
library(randomForest)
rf=randomForest(V65~.,data=g,mtry=8,importance=TRUE)
varImpPlot(rf)
```

```{r}
## Variable Selection by stepwise logistic regression based on AIC/BIC


#min.model = multinom(V65~1, data=g[train,])
#biggest = multinom(V65~., data=g[train,])
#fwd1.model = step(min.model, scope=formula(biggest), direction="forward")
#summary(fwd1.model)
#pred_lr1 = predict(fwd1.model, g[-train,])
#mean(pred_lr1 == g[-train,65])
#fwd2.model = step(min.model, scope=formula(biggest), direction="forward", k=log(length(train)))
#summary(fwd2.model)
```

## 4. Classification Methods

```{r}
## Split the dataset into training and test sets
set.seed(77)
g_new=g[,c(seq(from=2,to=58,by=8),seq(from=7,to=63,by=8),65)]
train=sample(1:nrow(g_new),size=nrow(g_new)*0.8,replace=FALSE)
```


### 4.1 Logistic Regression


```{r}
## Multinomal Logistic Regression
set.seed(3)
library(nnet)
lr_fit = multinom(V65~., data=g_new[train,])
lr_pred = predict(lr_fit, g_new[-train,])
accuracy_lr = mean(lr_pred == g_new[-train,'V65'])
```

### 4.2 LDA

```{r}
## LDA
library(MASS)
lda_fit = lda(V65~., data=g_new[train,])
lda_pred = predict(lda_fit, g_new[-train,])
accuracy_lda = mean(lda_pred$class == g_new[-train,'V65'])
```

### 4.3 QDA

```{r}
## QDA
qda_fit = qda(V65~., data = g_new[train,])
qda_pred = predict(qda_fit, g_new[-train,])$class
accuracy_qda = mean(qda_pred==g_new[-train,'V65'])
```



### 4.4 KNN

```{r}
library(class)
## Initialize
knn_cv_error = NULL
## Possible k's
k_list = 1:20
set.seed(3)
## Cross Validation function for knn
knn.cv = function(k, t, nfolds=5) {
  n_train = nrow(t)
  ## Split training and validation sets
  s = split(sample(n_train), rep(1:nfolds, length=n_train))
  cv_error = 0
  for(i in seq(nfolds)){
    ## Computing validation errors
    knn_cv_pred = knn(t[-s[[i]],-17], t[s[[i]],-17], t[-s[[i]],17], k=k)
    cv_error = cv_error + mean(knn_cv_pred!=t[s[[i]],17])
  }
  cv_error = cv_error / nfolds
}

## Perform cross validation
## choose k=3
for(k in k_list) {
  knn_cv_error = c(knn_cv_error, knn.cv(k, g_new[train,]))
}
```


```{r, fig.cap=cap, fig.align = "center"}
## Plot validation errors
cap = 'The 5-fold Cross Validation Errors for Each Choice of K.'
plot(k_list, knn_cv_error, type='l', ylim=c(0.13, 0.23),
     xlab='Number of Nearest Neighbors K', ylab='Cross Validation Error')
points(k_list[which.min(knn_cv_error)], min(knn_cv_error))
text(k_list[which.min(knn_cv_error)], min(knn_cv_error)-0.01, "3")
```

Choose K=3.

```{r}
## Predicting by knn and k=3
set.seed(3)
knn_pred = knn(g_new[train,-17], g_new[-train,-17], g_new[train,17], k=3)
accuracy_knn = mean(knn_pred == g_new[-train,17])
```

### 4.5 SVM

Validation to choose the optimal parameters

```{r}
## SVM
set.seed(77)
library(e1071)
## The validation set 
valid=sample(train,size=floor(length(train)*0.2),replace=FALSE)
train2=setdiff(train,valid)
## Possible parameters
costv=c(0.01,0.1,1,10,100)
gammav=c(0.5,1,2,3,4)
svm_v_error=cbind(expand.grid(costv,gammav),0)
## Perform validation set method to choose parameters
for (i in 1:nrow(svm_v_error)){
  svm_v_fit=svm(V65~.,data=g_new[train2,], kernel="radial", cost=svm_v_error[i,1],gamma=svm_v_error[i,2])
  svm_v_pred=predict(svm_v_fit,g_new[valid,])
  svm_v_error[i,3]=mean(svm_v_pred!=g_new[valid,"V65"])
}

svm_v_error[which.min(svm_v_error[,3]),]
```

So, choose cost=1, gamma=0.5.

```{r}
## Fit svm with cost=1, gamma=0.5
svm_fit=svm(V65~.,data=g_new[train,], kernel="radial", cost=1,gamma=0.5)
svm_pred=predict(svm_fit,g_new[-train,])
accuracy_svm=mean(svm_pred==g_new[-train,"V65"])
accuracy_svm
```

### 4.6 Classification Tree
```{r}
## Classification Tree
library(tree)
tree_fit=tree(V65~.,data=g_new[train,])
tree_pred=predict(tree_fit,g_new[-train,],type="class")
accuracy_tree=mean(tree_pred==g_new$V65[-train])
```

```{r}
plot(tree_fit)
text(tree_fit,pretty=0,cex=0.5)
tree_fit
```


### 4.7 Random Forest

```{r}
set.seed(77)
rf_fit=randomForest(V65~.,data=g_new[train,],mtry=4,importance=TRUE)
rf_pred = predict(rf_fit,newdata=g_new[-train,])
accuracy_rf=mean(rf_pred==g_new$V65[-train])
```


## 5 Prediction Accuracy Comparison

```{r}
t2=data.frame(method=c("Logistic Regression","LDA","QDA","KNN","SVM (radial)","Classification Tree","Random Forest"),
              pred=c(accuracy_lr,accuracy_lda,accuracy_qda,accuracy_knn,accuracy_svm,accuracy_tree,accuracy_rf))
cap="Prediction Accuracy of several methods"
knitr::kable(t2,format='pandoc',caption=cap,align='c',
             col.names=c('Method','Prediction Accuracy'))
```


## 6 Conclusion and Discussion 


